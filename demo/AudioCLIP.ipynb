{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bbf255",
   "metadata": {},
   "source": [
    "# AudioCLIP Demo\n",
    "\n",
    "Authored by [Andrey Guzhov](https://github.com/AndreyGuzhov)\n",
    "\n",
    "This notebook covers common use cases of AudioCLIP and provides the typical workflow.\n",
    "Below, you will find information on:\n",
    "\n",
    "0. [Binary Assets](#Downloading-Binary-Assets)\n",
    "1. [Required imports](#Imports-&-Constants)\n",
    "2. [Model Instantiation](#Model-Instantiation)\n",
    "3. [Data Transformation](#Audio-&-Image-Transforms)\n",
    "4. Data Loading\n",
    "    * [Audio](#Audio-Loading)\n",
    "    * [Images](#Image-Loading)\n",
    "5. [Preparation of the Input](#Input-Preparation)\n",
    "6. [Acquisition of the Output](#Obtaining-Embeddings)\n",
    "7. [Normalization of Embeddings](#Normalization-of-Embeddings)\n",
    "8. [Calculation of Logit Scales](#Obtaining-Logit-Scales)\n",
    "9. [Computation of Similarities](#Computing-Similarities)\n",
    "10. Performing Tasks\n",
    "    1. [Classification](#Classification)\n",
    "        1. [Audio](#Audio)\n",
    "        2. [Images](#Images)\n",
    "    2. [Querying](#Querying)\n",
    "        1. [Audio by Text](#Audio-by-Text)\n",
    "        2. [Images by Text](#Images-by-Text)\n",
    "        3. [Audio by Images](#Audio-by-Images)\n",
    "        4. [Images by Audio](#Images-by-Audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7262c7e",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70dbdf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import simplejpeg\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.append(os.path.abspath(f'{os.getcwd()}/..')) # .ipynb 파일이 위치한 경로\n",
    "\n",
    "from model import AudioCLIP\n",
    "from utils.transforms import ToTensor1D\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "MODEL_FILENAME = 'AudioCLIP-Full-Training.pt'\n",
    "# derived from ESResNeXt\n",
    "SAMPLE_RATE = 44100\n",
    "# derived from CLIP\n",
    "IMAGE_SIZE = 224\n",
    "IMAGE_MEAN = 0.48145466, 0.4578275, 0.40821073\n",
    "IMAGE_STD = 0.26862954, 0.26130258, 0.27577711\n",
    "\n",
    "labels_path = '/home/broiron/Desktop/AudioCLIP/data/label/labels_8.txt' # test를 위한 label\n",
    "with open(labels_path, 'r') as file:\n",
    "    labels = [line.strip() for line in file]\n",
    "# LABELS = ['cat', 'thunderstorm', 'coughing', 'alarm clock', 'car horn'] # audio 개수와 일치하지 않아도 되는건가?, 학습 시에는 맞춰줘야 함(train /val split 때문에)\n",
    "# print(type(label))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a327cc6",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f398f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclp = AudioCLIP(pretrained=f'../assets/{MODEL_FILENAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39421f88",
   "metadata": {},
   "source": [
    "## Audio & Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd4d76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_transforms = ToTensor1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5beab9",
   "metadata": {},
   "source": [
    "## Audio Loading\n",
    "Audio samples are drawn from the [ESC-50](https://github.com/karolpiczak/ESC-50) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aaa79b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/test_2/toilet flushing_102985_segment_3.wav', '../data/test_2/people sobbing_107583_segment_3.wav', '../data/test_2/air conditioning noise_389_segment_0.wav', '../data/test_2/railroad car, train wagon_73571_segment_2.wav', '../data/test_2/people whispering_13617_segment_2.wav', '../data/test_2/pumping water_37963_segment_4.wav', '../data/test_2/children shouting_31622_segment_1.wav', '../data/test_2/race car, auto racing_48786_segment_2.wav']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "paths_to_audio = glob.glob('../data/test_2/*.wav') # audio 파일이 위치한 경로\n",
    "\n",
    "audio = list()\n",
    "for path_to_audio in paths_to_audio:\n",
    "    track, _ = librosa.load(path_to_audio, sr=SAMPLE_RATE, dtype=np.float32)\n",
    "\n",
    "    # compute spectrograms using trained audio-head (fbsp-layer of ESResNeXt)\n",
    "    # thus, the actual time-frequency representation will be visualized\n",
    "    spec = aclp.audio.spectrogram(torch.from_numpy(track.reshape(1, 1, -1)))\n",
    "    spec = np.ascontiguousarray(spec.numpy()).view(np.complex64)\n",
    "    pow_spec = 10 * np.log10(np.abs(spec) ** 2 + 1e-18).squeeze()\n",
    "\n",
    "    audio.append((track, pow_spec))\n",
    "\n",
    "print(paths_to_audio) # 어떤 순서로 loading 되는지는 잘 모르겠음\n",
    "print(len(paths_to_audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c11976",
   "metadata": {},
   "source": [
    "## Input Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbf1059b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['air conditioning noise'], ['children shouting'], ['people sobbing'], ['people whispering'], ['pumping water'], ['race car, auto racing'], ['railroad car, train wagon'], ['toilet flushing']]\n"
     ]
    }
   ],
   "source": [
    "# AudioCLIP handles raw audio on input, so the input shape is [batch x channels x duration]\n",
    "audio = torch.stack([audio_transforms(track.reshape(1, -1)) for track, _ in audio])\n",
    "# text\n",
    "text = [[label] for label in labels]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb9ef5",
   "metadata": {},
   "source": [
    "## Obtaining Embeddings\n",
    "For the sake of clarity, all three modalities are processed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "568f4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text encoding \n",
    "from model.model_final import FrozenCLIPTextEmbedder, Mapping_Model # text encoder\n",
    "text_encoder = FrozenCLIPTextEmbedder(version='RN50', device=device)\n",
    "\n",
    "#text_features = torch.stack([text_encoder.encode([label]).to(device).float() for label in LABELS])\n",
    "#text_embeddings = torch.stack([text_encoder.encode([label]).to(device).float() for label in labels_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60c71e0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 2048, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# AudioCLIP's output: Tuple[Tuple[Features, Logits], Loss]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Features = Tuple[AudioFeatures, ImageFeatures, TextFeatures]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Logits = Tuple[AudioImageLogits, AudioTextLogits, ImageTextLogits]\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m ((audio_features, _, _), _), _ \u001b[38;5;241m=\u001b[39m \u001b[43maclp\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# audio embedding \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(audio_features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ((_, _, text_features), _), _ = aclp(text=text) # text embedding\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioclip/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/AudioCLIP/model/audioclip.py:205\u001b[0m, in \u001b[0;36mAudioCLIP.forward\u001b[0;34m(self, audio, image, text, batch_indices)\u001b[0m\n\u001b[1;32m    202\u001b[0m sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     audio_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# audio_features = audio_features / audio_features.norm(dim=-1, keepdim=True)\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/AudioCLIP/model/audioclip.py:165\u001b[0m, in \u001b[0;36mAudioCLIP.encode_audio\u001b[0;34m(self, audio)\u001b[0m\n\u001b[1;32m    162\u001b[0m     audio_tensor \u001b[38;5;241m=\u001b[39m audio_tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# 오디오 인코딩 수행\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     audio_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 배치 차원 추가\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     encoded_audios\u001b[38;5;241m.\u001b[39mappend(audio_embedding)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# 인코딩된 오디오 특징들을 하나의 텐서로 결합\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioclip/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/AudioCLIP/model/esresnet/base.py:393\u001b[0m, in \u001b[0;36mResNetWithAttention.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    391\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_processing(x)\n\u001b[1;32m    392\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_features(x)\n\u001b[0;32m--> 393\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_classifier(x)\n\u001b[1;32m    396\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AudioCLIP/model/esresnet/base.py:630\u001b[0m, in \u001b[0;36m_ESResNet._forward_reduction\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    628\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[0;32m--> 630\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_ESResNet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m    632\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/AudioCLIP/model/esresnet/base.py:373\u001b[0m, in \u001b[0;36mResNetWithAttention._forward_reduction\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    371\u001b[0m     x_att \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    372\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m--> 373\u001b[0m     x_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_att\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m x_att\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/audioclip/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/AudioCLIP/model/esresnet/attention.py:37\u001b[0m, in \u001b[0;36mAttention2d.forward\u001b[0;34m(self, x, size)\u001b[0m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_depth(x)\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_point(x)\n\u001b[0;32m---> 37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/audioclip/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/audioclip/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioclip/lib/python3.8/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2448\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/audioclip/lib/python3.8/site-packages/torch/nn/functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 2048, 1, 1])"
     ]
    }
   ],
   "source": [
    "# AudioCLIP's output: Tuple[Tuple[Features, Logits], Loss]\n",
    "# Features = Tuple[AudioFeatures, ImageFeatures, TextFeatures]\n",
    "# Logits = Tuple[AudioImageLogits, AudioTextLogits, ImageTextLogits]\n",
    "\n",
    "((audio_features, _, _), _), _ = aclp(audio=audio) # audio embedding \n",
    "print(audio_features.shape)\n",
    "# ((_, _, text_features), _), _ = aclp(text=text) # text embedding\n",
    "text_embeddings = torch.stack(text_encoder.encode([text]).to(device).float())\n",
    "for i in text_embeddings: \n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e45ed0",
   "metadata": {},
   "source": [
    "## Normalization of Embeddings\n",
    "The AudioCLIP's output is normalized using L<sub>2</sub>-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9758c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nomalized audio embedding shape: torch.Size([10, 1024])\n",
      "tensor([[-0.0054,  0.0168, -0.0171,  ..., -0.0010, -0.0028,  0.0119],\n",
      "        [ 0.0124, -0.0051, -0.0022,  ..., -0.0160,  0.0204, -0.0063],\n",
      "        [-0.0062, -0.0080, -0.0636,  ..., -0.0152,  0.0046, -0.0099],\n",
      "        [-0.0233, -0.0438, -0.0098,  ..., -0.0057, -0.0173,  0.0046],\n",
      "        [ 0.0153, -0.0070, -0.0421,  ...,  0.0153, -0.0002, -0.0064]])\n"
     ]
    }
   ],
   "source": [
    "audio_features = audio_features / torch.linalg.norm(audio_features, dim=-1, keepdim=True)\n",
    "print(f'nomalized audio embedding shape: {audio_features.shape}') # normalization 한다고 해서 차원이 달라지진 않음\n",
    "text_features = text_features / torch.linalg.norm(text_features, dim=-1, keepdim=True)\n",
    "print(text_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92adfb5",
   "metadata": {},
   "source": [
    "## Obtaining Logit Scales\n",
    "Outputs of the text-, image- and audio-heads are made consistent using dedicated scaling terms for each pair of modalities.\n",
    "The scaling factors are clamped between 1.0 and 100.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a89e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(100.)\n"
     ]
    }
   ],
   "source": [
    "# audio와 text만 고려\n",
    "scale_audio_text = torch.clamp(aclp.logit_scale_at.exp(), min=1.0, max=100.0)\n",
    "# print(f'scaled embedding: {scale_audio_text}')\n",
    "print(scale_audio_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3dfd0",
   "metadata": {},
   "source": [
    "## Computing Similarities\n",
    "Similarities between different representations of a same concept are computed using [scaled](#Obtaining-Logit-Scales) dot product (cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3121148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.7929, -0.5249,  3.7583,  1.0434,  6.3707],\n",
      "        [ 7.0164,  0.4689, -0.3424, -0.4053,  3.7883],\n",
      "        [ 9.2904, -1.1262, -5.1571, -4.6368,  0.7858],\n",
      "        [ 1.6267,  1.0276,  2.8426,  0.4084,  5.9047],\n",
      "        [-0.6302,  0.2372,  6.4231,  1.0683,  0.6146],\n",
      "        [ 1.9733,  7.2482,  1.7210,  0.0743,  2.7171],\n",
      "        [ 2.4561,  2.8463,  2.7906,  1.4245,  5.3515],\n",
      "        [ 3.1987,  2.7979,  0.6837,  8.2739,  3.6359],\n",
      "        [ 2.7705,  3.0641,  0.2023,  8.5307,  3.8250],\n",
      "        [ 1.7586,  7.9345, -1.5157, -1.7790,  1.9068]])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# audio와 text 간의 similality 계산\n",
    "logits_audio_text = scale_audio_text * audio_features @ text_features.T\n",
    "print(logits_audio_text)\n",
    "print(logits_audio_text.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0dfa2",
   "metadata": {},
   "source": [
    "## Classification\n",
    "This task is a specific case of a more general one, which is [querying](#Querying).\n",
    "However, this setup is mentioned as a standalone because it demonstrates clearly how to perform usual classification (including [zero-shot inference](https://github.com/openai/CLIP#zero-shot-prediction)) using AudioCLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd15af",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc74da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tFilename, Audio\t\t\tTextual Label (Confidence)\n",
      "\n",
      "coughing_1-58792-A-24_segment_1.wav ->\t\t       car horn (86.57%),             cat (06.57%),        coughing (06.35%)\n",
      " cat_3-95694-A-5_segment_0.wav ->\t\t            cat (95.94%),        car horn (03.80%),    thunderstorm (00.14%)\n",
      " cat_3-95694-A-5_segment_1.wav ->\t\t            cat (99.98%),        car horn (00.02%),    thunderstorm (00.00%)\n",
      "car_horn_1-24074-A-43_segment_1.wav ->\t\t       car horn (93.25%),        coughing (04.36%),             cat (01.29%)\n",
      "coughing_1-58792-A-24_segment_0.wav ->\t\t       coughing (98.95%),     alarm clock (00.47%),        car horn (00.30%)\n",
      "thunder_3-144891-B-19_segment_1.wav ->\t\t   thunderstorm (97.98%),        car horn (01.06%),             cat (00.50%)\n",
      "car_horn_1-24074-A-43_segment_0.wav ->\t\t       car horn (81.05%),    thunderstorm (06.62%),        coughing (06.26%)\n",
      "alarm_clock_3-120526-B-37_segment_0.wav ->\t\t    alarm clock (97.98%),        car horn (00.95%),             cat (00.61%)\n",
      "alarm_clock_3-120526-B-37_segment_1.wav ->\t\t    alarm clock (98.36%),        car horn (00.89%),    thunderstorm (00.42%)\n",
      "thunder_3-144891-B-19_segment_0.wav ->\t\t   thunderstorm (99.54%),        car horn (00.24%),             cat (00.21%)\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\tFilename, Audio\\t\\t\\tTextual Label (Confidence)', end='\\n\\n')\n",
    "\n",
    "# calculate model confidence\n",
    "confidence = logits_audio_text.softmax(dim=1)\n",
    "# print(f'confidence score: {confidence}')\n",
    "\n",
    "for audio_idx in range(len(paths_to_audio)):\n",
    "    # acquire Top-3 most similar results\n",
    "    conf_values, ids = confidence[audio_idx].topk(3) # ids의 type은 tensor\n",
    "\n",
    "    # format output strings\n",
    "    query = f'{os.path.basename(paths_to_audio[audio_idx]):>30s} ->\\t\\t'\n",
    "    results = ', '.join([f'{LABELS[i]:>15s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "    # print(conf_values) # 결과마다 다 다른 tensor 값이 나와야 함. -> 제대로 된 결과\n",
    "\n",
    "    print(query + results)\n",
    "\n",
    "    # classification 결과로 봤을 때, data의 개수와 label의 개수가 일치하지 않아도 되는 것인가?\n",
    "    # 즉, 중복을 제거하고 각각 하나의 label만 존재해도 되는듯 싶음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a1a06",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939334c",
   "metadata": {},
   "source": [
    "### Audio by Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0b4ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTextual Label\t\tFilename, Audio (Confidence)\n",
      "\n",
      "                      cat ->\t\t cat_3-95694-A-5_segment_1.wav (89.80%),  cat_3-95694-A-5_segment_0.wav (09.24%)\n",
      "             thunderstorm ->\t\tthunder_3-144891-B-19_segment_0.wav (65.55%), thunder_3-144891-B-19_segment_1.wav (33.00%)\n",
      "                 coughing ->\t\tcoughing_1-58792-A-24_segment_0.wav (87.74%), coughing_1-58792-A-24_segment_1.wav (06.11%)\n",
      "              alarm clock ->\t\talarm_clock_3-120526-B-37_segment_1.wav (56.30%), alarm_clock_3-120526-B-37_segment_0.wav (43.55%)\n",
      "                 car horn ->\t\tcoughing_1-58792-A-24_segment_1.wav (44.41%), car_horn_1-24074-A-43_segment_1.wav (27.87%)\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\tTextual Label\\t\\tFilename, Audio (Confidence)', end='\\n\\n')\n",
    "\n",
    "# calculate model confidence\n",
    "confidence = logits_audio_text.softmax(dim=0)\n",
    "for label_idx in range(len(LABELS)):\n",
    "    # acquire Top-2 most similar results\n",
    "    conf_values, ids = confidence[:, label_idx].topk(2)\n",
    "\n",
    "    # format output strings\n",
    "    query = f'{LABELS[label_idx]:>25s} ->\\t\\t'\n",
    "    results = ', '.join([f'{os.path.basename(paths_to_audio[i]):>30s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "    print(query + results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
